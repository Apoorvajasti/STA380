cv.ridge=cv.glmnet(train.matrix,train.response,alpha=0,lambda=grid)
plot(cv.ridge)
library(glmnet)
cv.ridge=cv.glmnet(train.matrix,train.response,alpha=0,lambda=grid)
plot(cv.ridge)
points(log(cv.ridge$lambda.min),cv.ridge$cvm[cv.ridge$lambda==cv.ridge$lambda.min], col = "blue", cex = 2, pch = 20)
best.lambda <- cv.ridge$lambda.min
print(paste('Using cross-validation, value of lambda that gives minimum mean cross-validated error is',as.character(best.lambda),sep=" "))
ridge.pred.best=predict(cv.ridge,s=best.lambda,newx=test.matrix)
test.error.ridge <- sqrt(mean((test.response - ridge.pred.best)^2))
print(paste("The test error using ridge regression is",as.character(test.error.ridge)," when value of lambda is",as.character(best.lambda),sep=' '))
set.seed(1892)
cv.lasso=cv.glmnet(train.matrix,train.response,alpha=1,lambda=grid)
points(log(cv.lasso$lambda.min),cv.lasso$cvm[cv.lasso$lambda==cv.lasso$lambda.min], col = "blue", cex = 2, pch = 20)
coef.lasso <- coef(glmnet(train.matrix,train.response,alpha=1,lambda=best.lambda))
print("Coefficients of Lasso Regression Model")
coef.lasso
best.lambda <- cv.lasso$lambda.min
print(paste('Using cross-validation, value of lambda that gives minimum mean cross-validated error is',as.character(best.lambda),sep=" "))
lasso.pred.best=predict(cv.lasso,s=best.lambda,newx=test.matrix)
test.error.lasso <- sqrt(mean((test.response - lasso.pred.best)^2))
print(paste("The test error using lasso regression is",as.character(test.error.lasso),"when value of lambda is",as.character(best.lambda),sep=' '))
print(paste("The number of non-zero coefficients are",as.character(length(coef.lasso[which(coef.lasso != 0 ) ])-1),"when lambda =",as.character(best.lambda),sep=' '))
print(paste("The number of non-zero coefficients are",as.character(length(coef.lasso[which(coef.lasso != 0 ) ])-1),"when lambda =",as.character(best.lambda),sep=' '))
lm.fit.lasso.select <- lm(crim~zn+indus+chas+nox+dis+rad+ptratio+black+lstat+medv)
lm.fit.lasso.select.error <- sqrt(mean((test.response - predict(lm.fit.lasso.select, test))^2))
print(paste("The test error using least squares regression with non-zero predictors from lasso is",as.character(lm.fit.lasso.select.error),sep=' '))
pcr.fit=pcr(crim~.,data=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type="RMSEP")
summary(pcr.fit)
pcr.pred=predict(pcr.fit,test,ncomp=8)
library(pls)
pcr.fit=pcr(crim~.,data=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type="RMSEP")
summary(pcr.fit)
pcr.pred=predict(pcr.fit,test,ncomp=8)
test.error.pcr <- sqrt(mean((pcr.pred-test.response)^2))
print(paste("The test error using PCR is",as.character(test.error.pcr)," when value of M is 8",sep=' '))
plsr.fit=plsr(crim~.,data=train,scale=TRUE,validation="CV")
validationplot(plsr.fit,val.type="RMSEP")
summary(plsr.fit)
test.error.pls <- sqrt(mean((plsr.pred-test.response)^2))
plsr.pred=predict(plsr.fit,test,ncomp=6)
test.error.pls <- sqrt(mean((plsr.pred-test.response)^2))
print(paste("The test error using PLS is",as.character(test.error.pcr)," when value of M is 6",sep=' '))
columns <- c("Model","Test RMSE")
rows <- c("Multiple Regression","Selected Multiple Regression","Full Subset Regression","Ridge Regression","Lasso Regression","Mutiple Regression using predictors from Lasso","PCR","PLS")
values <- c(test.error.lm,test.error.lm.select,test.error.lm.subset,test.error.ridge,test.error.lasso,lm.fit.lasso.select.error,test.error.pcr,test.error.pls)
df.error <- cbind(rows,values)
colnames(df.error) <- columns
pander(df.error)
print(paste("All the models are giving similar test errors with",df.error[df.error[,2]==min(df.error[,2])][1], "giving the lowest error.",sep=' '))
library(pander)
columns <- c("Model","Test RMSE")
rows <- c("Multiple Regression","Selected Multiple Regression","Full Subset Regression","Ridge Regression","Lasso Regression","Mutiple Regression using predictors from Lasso","PCR","PLS")
values <- c(test.error.lm,test.error.lm.select,test.error.lm.subset,test.error.ridge,test.error.lasso,lm.fit.lasso.select.error,test.error.pcr,test.error.pls)
df.error <- cbind(rows,values)
colnames(df.error) <- columns
pander(df.error)
print(paste("All the models are giving similar test errors with",df.error[df.error[,2]==min(df.error[,2])][1], "giving the lowest error.",sep=' '))
reg.full.summary
summary(lm.fit.subset)
library(BART)
library(gbm)
library(randomForest)
###Data Prep
data <- read.csv("/Users/tushargupta/Documents/UT Austin/Predictive Modeling/Take Home Test/Data/CAhousing.csv")
data$AveBedrms <- data$totalBedrooms/data$households
data$AveRooms <- data$totalRooms/data$households
data$AveOccupancy <- data$population/data$households
logMedVal <- log(data$medianHouseValue)
data <- data[,-c(4,5,9)] # lose lmedval and the room totals
data$logMedVal = logMedVal
x = data[,c(1:9)]
y = data$logMedVal # House
#train, val, test
set.seed(109)
n=nrow(data)
#creating training and testing data
train = sample(1:nrow(data),round(0.75*nrow(data)))
xtrain=x[train,];
ytrain=y[train] # training data
xtest=x[-train,];
ytest=y[-train] # test data
set.seed(1892)
barttrain = wbart(xtrain,ytrain)
yhat = predict(barttrain,as.matrix(xtest))
yhat.mean = apply(yhat,2,mean)
plot(ytest,yhat.mean)
abline(0,1,col=2)
error.bart = (mean((ytest-yhat.mean)^2))^(1/2)
print(paste("Test RMSE of BART is",error.bart))
#### Boosting
set.seed(1892)
catrainval = rbind(catrain,caval)
ntrees=5000
finb = gbm(logMedVal~.,data=catrainval,distribution='gaussian',
interaction.depth=4,n.trees=ntrees,shrinkage=.2)
finbpred=predict(finb,newdata=catest,n.trees=ntrees)
catrainval = rbind(catrain,caval)
catrainval = rbind(xtrain,ytrain)
ntrees=5000
finb = gbm(logMedVal~.,data=catrainval,distribution='gaussian',
interaction.depth=4,n.trees=ntrees,shrinkage=.2)
catrainval = rbind(xtrain,ytrain)
ntrees=5000
finb = gbm(logMedVal~.,data=catrainval,distribution='gaussian',
interaction.depth=4,n.trees=ntrees,shrinkage=.2)
dim(xtrain)
dim(ytrain)
ytrain=y[train] # training data
dim(ytrain)
length(ytrain)
sim(catrainval)
dim(catrainval)
catrainval = cbind(catrain,caval)
dim(catrainval)
catrainval = cbind(xtrain,ytrain)
dim(catrainval)
ntrees=5000
finb = gbm(logMedVal~.,data=catrainval,distribution='gaussian',
interaction.depth=4,n.trees=ntrees,shrinkage=.2)
catrainval
finb = gbm(ytrain~.,data=catrainval,distribution='gaussian',
interaction.depth=4,n.trees=ntrees,shrinkage=.2)
finbpred=predict(finb,newdata=xtest,n.trees=ntrees)
error.boost <- sqrt(mean((finbpred-ytest)^2))
print(paste("Test RMSE of Boosting is",error.boost))
#### RandomForest
set.seed(1892)
finrf = randomForest(ytrain~.,data=catrainval,mtry=3,ntree=500)
finrfpred=predict(finrf,newdata=xtest)
error.rf = sqrt(mean((ytrain-finrfpred)^2))
print(paste("Test RMSE of Random Forest is",error.rf))
catrainval = cbind(xtrain,ytrain)
finrf = randomForest(ytrain~.,data=catrainval,mtry=3,ntree=5000)
finrfpred=predict(finrf,newdata=xtest)
error.rf = sqrt(mean((ytrain-finrfpred)^2))
print(paste("Test RMSE of Random Forest is",error.rf))
library(stylo)
setwd("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/Author Attribution/ReutersC50/")
folder_list = Sys.glob('C50train/*')
file_list_train=c()
## creating file_list_train and labels_train
for (f in folder_list){
file_list_train=c(file_list_train,Sys.glob(paste(f, '/*', sep="")))
}
raw.corpus <- load.corpus(files=file_list_train)
tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
preserve.case = FALSE)
corpus.no.pronouns <- delete.stop.words(tokenized.corpus,
stop.words = stylo.pronouns(language = "English"))
corpus.char.1.grams <- txt.to.features(corpus.no.pronouns, ngram.size = 1,
features = "w")
corpus.char.2.grams <- txt.to.features(corpus.no.pronouns, ngram.size = 2,
features = "w")
frequent.features <- c(make.frequency.list(corpus.char.1.grams),make.frequency.list(corpus.char.2.grams))
freqs <- make.table.of.frequencies(tokenized.corpus,features=frequent.features,relative=FALSE)
frequent.features <- c(make.frequency.list(corpus.char.1.grams),make.frequency.list(corpus.char.2.grams,head=100000))
length(frequent.features)
frequent.features <- c(make.frequency.list(corpus.char.1.grams),make.frequency.list(corpus.char.2.grams))
length(frequent.features)
x = make.frequency.list(corpus.char.1.grams)
length(x)
frequent.features <- c(make.frequency.list(corpus.char.1.grams),make.frequency.list(corpus.char.2.grams,head=100000))
freqs <- make.table.of.frequencies(tokenized.corpus,features=frequent.features,relative=FALSE)
freqs
frequent.features <- c(make.frequency.list(corpus.char.1.grams),make.frequency.list(corpus.char.2.grams,head=10))
frequent.features
frequent.features[:length(frequent.features)]
frequent.features[length(frequent.features)-3:length(frequent.features)]
length(frequent.features)-3
frequent.features[32405]
frequent.features[32405:32407]
frequent.features[32405:32409]
freqs <- make.table.of.frequencies(tokenized.corpus,features=frequent.features,relative=FALSE)
frequent.features <- make.frequency.list(corpus.char.1.grams)
freqs <- make.table.of.frequencies(tokenized.corpus,features=frequent.features,relative=FALSE)
freqs
frequent.features <- make.frequency.list(corpus.char.2.grams,head=100)
frequent.features
tokenized.corpus
freqs <- make.table.of.frequencies(raw.corpus,features=frequent.features,relative=FALSE)
my.vector.of.words = corpus.no.pronouns(my.text)
my.vector.of.words = txt.to.words(corpus.no.pronouns)
my.vector.of.words
my.vector.of.words = txt.to.words(corpus.no.pronouns,ngram.size=2)
x = make.ngrams(corpus.no.pronouns, ngram.size = 2)
x
library(RWeka)
library(RWeka)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(LICORS)
library(janitor)
library(knitr)
library(kableExtra)
library(ggalt)
setwd("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/")
data = read.csv("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/Market Segmentation/social_marketing.csv",row.names=1)
total_col = apply(data,1,sum)
total_col = apply(data,1,sum)
rel_freq = function(x) {
x / total_col
}
data.1 = data %>% mutate(rel_uncat=rel_freq(uncategorized),rel_adult=rel_freq(adult))
data.clean = data.1 %>% filter(rel_uncat<.33&rel_adult<.33)
data.clean = data.clean[,!(names(data.clean) %in% c("rel_uncat","rel_adult"))]
data.clean = scale(data.clean,center=TRUE,scale=TRUE)
mu = attr(data.clean,"scaled:center")
sigma = attr(data.clean,"scaled:scale")
cluster = kmeans(data.clean,4,nstart=20)
cluster$size/nrow(data.clean)
cluster = kmeans(data.clean,3,nstart=20)
cluster$size/nrow(data.clean)
data.clean.1 = data.1 %>% filter(rel_uncat<.33&rel_adult<.33)
all_data = as.data.frame(cbind(data.clean.1,cluster$cluster))
names(all_data)[names(all_data) == 'cluster$cluster'] <- 'cluster'
all_data$cluster = as.factor(all_data$cluster)
cluster1 = all_data[all_data$cluster=='1',]
cluster2 = all_data[all_data$cluster=='2',]
cluster3 = all_data[all_data$cluster=='3',]
cluster1_rel = cluster1[,-37]/rowSums(cluster1[,-37])
Z = data.clean.1/rowSums(data.clean.1)
pc2 = prcomp(Z,scale=TRUE,rank=5,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
qplot(scores[,1], scores[,2], color=Z$cluster, xlab='Component 1', ylab='Component 2')+scale_color_colorblind()
library(dplyr)
library(ggplot2)
library(ggthemes)
library(LICORS)
library(janitor)
library(knitr)
library(kableExtra)
library(ggalt)
setwd("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/")
data = read.csv("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/Market Segmentation/social_marketing.csv",row.names=1)
total_col = apply(data,1,sum)
rel_freq = function(x) {
x / total_col
}
data.1 = data %>% mutate(rel_uncat=rel_freq(uncategorized),rel_adult=rel_freq(adult))
data.clean = data.1 %>% filter(rel_uncat<.33&rel_adult<.33)
data.clean = data.clean[,!(names(data.clean) %in% c("rel_uncat","rel_adult"))]
data.clean = scale(data.clean,center=TRUE,scale=TRUE)
mu = attr(data.clean,"scaled:center")
sigma = attr(data.clean,"scaled:scale")
set.seed(1892)
grid=seq(2,25,1)
sse = c()
ch = c()
for(i in grid){
cluster = kmeans(data.clean,i,nstart=20)
sse = c(sse,cluster$tot.withinss)
W = cluster$tot.withinss
B = cluster$betweenss
N = nrow(data.clean)
ch1 = (B/W)*((N-i)/(i-1))
ch = c(ch,ch1)
}
output = as.data.frame(cbind(grid,sse,ch))
ggplot(output) +
geom_point(aes(x=grid,y=sse)) +
labs(x='K',y='SSE')
ggplot(output) +
geom_point(aes(x=grid,y=ch)) +
labs(x='K',y='CH')
cluster = kmeans(data.clean,3,nstart=20)
cluster$size/nrow(data.clean)
cluster_pp = kmeanspp(data.clean,4,nstart=20)
cluster_pp$size/nrow(data.clean)
distance_matrix = dist(data.clean, method='euclidean')
hier_cluster = hclust(distance_matrix, method='average')
hier_cluster_10 = cutree(hier_cluster, k=10)
summary(factor(hier_cluster_10))
Z = data.clean.1/rowSums(data.clean.1)
data.clean.1 = data.1 %>% filter(rel_uncat<.33&rel_adult<.33)
data.clean.1 = data.clean.1[,!(names(data.clean.1) %in% c("rel_uncat","rel_adult"))]
#all_data = as.data.frame(cbind(data.clean.1,cluster$cluster))
#names(all_data)[names(all_data) == 'cluster$cluster'] <- 'cluster'
#all_data$cluster = as.factor(all_data$cluster)
#cluster1 = all_data[all_data$cluster=='1',]
#cluster2 = all_data[all_data$cluster=='2',]
#cluster3 = all_data[all_data$cluster=='3',]
#cluster4 = all_data[all_data$cluster=='4',]
#cluster1_rel = cluster1[,-37]/rowSums(cluster1[,-37])
Z = data.clean.1/rowSums(data.clean.1)
pc2 = prcomp(Z,scale=TRUE,rank=5,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
cluster = kmeans(data.clean,4,nstart=20)
cluster$size/nrow(data.clean)
Z = data.clean.1/rowSums(data.clean.1)
pc2 = prcomp(Z,scale=TRUE,rank=5,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
qplot(scores[,1], scores[,2], color=Z$cluster, xlab='Component 1', ylab='Component 2')+scale_color_colorblind()
o1 = order(loadings[,1], decreasing = TRUE)
colnames(Z)[head(o1,10)]
colnames(Z)[tail(o1,10)]
colnames(Z)[o1[20:25]]
o2 = order(loadings[,2], decreasing = TRUE)
colnames(Z)[head(o2,10)]
colnames(Z)[tail(o2,10)]
colnames(Z)[o2[20:25]]
total_col = apply(data,1,sum)
data = read.csv("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/Market Segmentation/social_marketing.csv",row.names=1)
total_col = apply(data,1,sum)
rel_freq = function(x) {
x / total_col
}
data.1 = data %>% mutate(rel_uncat=rel_freq(uncategorized),rel_adult=rel_freq(adult))
data.clean = data.1 %>% filter(rel_uncat<.33&rel_adult<.33)
data.clean = data.clean[,!(names(data.clean) %in% c("rel_uncat","rel_adult","uncategorized","adult","chatter","spam"))]
data.clean = scale(data.clean,center=TRUE,scale=TRUE)
mu = attr(data.clean,"scaled:center")
sigma = attr(data.clean,"scaled:scale")
set.seed(1892)
grid=seq(2,25,1)
sse = c()
ch = c()
for(i in grid){
cluster = kmeans(data.clean,i,nstart=20)
sse = c(sse,cluster$tot.withinss)
W = cluster$tot.withinss
B = cluster$betweenss
N = nrow(data.clean)
ch1 = (B/W)*((N-i)/(i-1))
ch = c(ch,ch1)
}
output = as.data.frame(cbind(grid,sse,ch))
ggplot(output) +
geom_point(aes(x=grid,y=sse)) +
labs(x='K',y='SSE')
ggplot(output) +
geom_point(aes(x=grid,y=ch)) +
labs(x='K',y='CH')
cluster = kmeans(data.clean,5,nstart=20)
cluster$size/nrow(data.clean)
cluster_pp = kmeanspp(data.clean,4,nstart=20)
cluster_pp = kmeanspp(data.clean,4,nstart=20)
cluster_pp$size/nrow(data.clean)
cluster_pp = kmeanspp(data.clean,5,nstart=20)
cluster_pp$size/nrow(data.clean)
data.clean.1 = data.1 %>% filter(rel_uncat<.33&rel_adult<.33)
data.clean.1 = data.clean.1[,!(names(data.clean.1) %in% c("rel_uncat","rel_adult"))]
Z = data.clean.1/rowSums(data.clean.1)
pc2 = prcomp(Z,scale=TRUE,rank=5,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
qplot(scores[,1], scores[,2], color=Z$cluster, xlab='Component 1', ylab='Component 2')+scale_color_colorblind()
o1 = order(loadings[,1], decreasing = TRUE)
colnames(Z)[head(o1,10)]
colnames(Z)[tail(o1,10)]
ggbiplot(pc2,ellipse=TRUE, groups=clusters$`cluster$cluster`)
library(ggbiplot)
ggbiplot(pc2,ellipse=TRUE, groups=clusters$`cluster$cluster`)
ggbiplot(pc2,ellipse=TRUE, groups=Z$cluster)
o1 = order(loadings[,1], decreasing = TRUE)
colnames(Z)[head(o1,10)]
colnames(Z)[tail(o1,10)]
colnames(Z)[o1[20:25]]
ggbiplot(pc2,ellipse=TRUE, groups=Z$cluster)
o2 = order(loadings[,2], decreasing = TRUE)
colnames(Z)[head(o2,10)]
colnames(Z)[tail(o2,10)]
colnames(Z)[o2[20:25]]
ggbiplot(pc2,ellipse=TRUE, groups=Z$cluster)
View(data.clean)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(LICORS)
library(janitor)
library(knitr)
library(kableExtra)
library(ggalt)
library(ggbiplot)
setwd("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/")
data = read.csv("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/Market Segmentation/social_marketing.csv",row.names=1)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(LICORS)
library(janitor)
library(knitr)
library(kableExtra)
library(ggalt)
library(ggbiplot)
setwd("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/")
data = read.csv("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/Market Segmentation/social_marketing.csv",row.names=1)
total_col = apply(data,1,sum)
rel_freq = function(x) {
x / total_col
}
data.1 = data %>% mutate(rel_uncat=rel_freq(uncategorized),rel_adult=rel_freq(adult))
data.clean = data.1 %>% filter(rel_uncat<.33&rel_adult<.33)
data.clean = data.clean[,!(names(data.clean) %in% c("rel_uncat","rel_adult","uncategorized","adult","chatter","spam"))]
data.scaled = scale(data.clean,center=TRUE,scale=TRUE)
mu = attr(data.scaled,"scaled:center")
sigma = attr(data.scaled,"scaled:scale")
set.seed(1892)
grid=seq(2,25,1)
sse = c()
ch = c()
for(i in grid){
cluster = kmeans(data.scaled,i,nstart=20)
sse = c(sse,cluster$tot.withinss)
W = cluster$tot.withinss
B = cluster$betweenss
N = nrow(data.scaled)
ch1 = (B/W)*((N-i)/(i-1))
ch = c(ch,ch1)
}
output = as.data.frame(cbind(grid,sse,ch))
ggplot(output) +
geom_point(aes(x=grid,y=sse)) +
labs(x='K',y='SSE')
ggplot(output) +
geom_point(aes(x=grid,y=ch)) +
labs(x='K',y='CH')
cluster = kmeans(data.scaled,5,nstart=20)
cluster$size/nrow(data.scaled)
Z = data.clean/rowSums(data.clean)
pc2 = prcomp(Z,scale=TRUE,rank=5,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
qplot(scores[,1], scores[,2], color=Z$cluster, xlab='Component 1', ylab='Component 2')+scale_color_colorblind()
cluster = kmeans(data.scaled,6,nstart=20)
cluster$size/nrow(data.scaled)
Z = data.clean/rowSums(data.clean)
pc2 = prcomp(Z,scale=TRUE,rank=6,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
qplot(scores[,1], scores[,2], color=Z$cluster, xlab='Component 1', ylab='Component 2')+scale_color_colorblind()
cluster = kmeans(data.scaled,3,nstart=20)
cluster$size/nrow(data.scaled)
Z = data.clean/rowSums(data.clean)
pc2 = prcomp(Z,scale=TRUE,rank=3,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
qplot(scores[,1], scores[,2], color=Z$cluster, xlab='Component 1', ylab='Component 2')+scale_color_colorblind()
Z = data.clean/rowSums(data.clean)
pc2 = prcomp(Z,scale=TRUE,rank=5,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
qplot(scores[,1], scores[,2], color=Z$cluster, xlab='Component 1', ylab='Component 2')+scale_color_colorblind()
cluster = kmeans(data.scaled,5,nstart=20)
cluster$size/nrow(data.scaled)
cluster = kmeans(data.scaled6,nstart=20)
cluster = kmeans(data.scaled,6,nstart=20)
cluster = kmeans(data.scaled,6,nstart=20)
cluster$size/nrow(data.scaled)
Z = data.clean/rowSums(data.clean)
pc2 = prcomp(Z,scale=TRUE,rank=5,center=TRUE)
summary(pc2)
loadings = pc2$rotation
scores = pc2$x
Z = as.data.frame(cbind(Z,cluster$cluster))
Z$`cluster$cluster` = as.factor(Z$`cluster$cluster`)
names(Z)[names(Z) == 'cluster$cluster'] <- 'cluster'
qplot(scores[,1], scores[,2], color=Z$cluster, xlab='Component 1', ylab='Component 2')+scale_color_colorblind()
biplot(pc2)
colnames(Z)[head(o1,25)]
# Question 2: how are the individual PCs loaded on the original variables?
# The top words associated with each component
colnames(Z)[order(loadings[,1], decreasing=TRUE)]
# Question 2: how are the individual PCs loaded on the original variables?
# The top words associated with each component
colnames(Z)[order(loadings[,1], decreasing=TRUE)]
colnames(Z)[order(loadings[,2], decreasing=TRUE)]
setwd("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/question 2/")
library(dplyr)
library(tidyr)
setwd("/Users/tushargupta/Documents/UT Austin/380/STA380/assignment/question 2/")
